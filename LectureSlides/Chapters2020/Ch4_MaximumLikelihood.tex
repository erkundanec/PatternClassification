\graphicspath{{../PR2018/Figures/}}

\title{\fontsize{33}{45}{\huge Pattern Classification (EET 3035)\newline \vspace{8pt} \Large Lecture 04\vspace{-1.1cm}}}
\author{\vspace{-0.4cm}\\\normalsize{\bf Dr. Kundan Kumar}\\ PhD (IIT Kharagpur)\\
Associate Professor\\Department of ECE}
% - Give the names in the same order as the appear in the paper.
% - Use the \inst{?} command only if the authors have different
%   affiliation.

\institute[Indian Institute of Technology Kharagpur] % (optional, but mostly needed)
{
\includegraphics[height=.17\textheight]{SOAlogo.png}\\
 Faculty of Engineering (ITER)\\ S`O'A Deemed to be University, Bhubaneswar, India-751030\\
 \copyright\  2020 Kundan Kumar, All Rights Reserved\\
  \vspace{-1.1cm}}
% - Use the \inst command only if there are several affiliations.
% - Keep it simple, no one is interested in your street address.
\date{}
% To remove page number from a perticular slide
{
\setbeamertemplate{logo}{}
\makeatletter
\setbeamertemplate{footline}{
        \leavevmode%
  
  % First line.
  \hbox{%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=\beamer@decolines@lineup,dp=0pt]{}%
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.8\paperwidth,ht=\beamer@decolines@lineup,dp=0pt]{lineup}%
  \end{beamercolorbox}%
  } %
  % Second line.
  \hbox{%
  \begin{beamercolorbox}[wd=\paperwidth,ht=\beamer@decolines@linemid,dp=0pt]{linemid}%
  \end{beamercolorbox}%
  } %
  % Third line.
  \hbox{%
  \begin{beamercolorbox}[wd=.1\paperwidth,ht=\beamer@decolines@linebottom,dp=0pt]{}%
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.9\paperwidth,ht=\beamer@decolines@linebottom,dp=0pt]{linebottom}%
  \end{beamercolorbox}%
  }%
        }
\makeatother
\begin{frame}
\titlepage
\end{frame}
}

\section{Parametric Estimation}
\subsection{}

\begin{frame}{}
\begin{variableblock}{\centering \Large \textbf{\vspace{4pt}\newline Parametric Estimation Techniques\vspace{4pt}}}{bg=slidecolor,fg=white}{bg=slidecolor,fg=white}
\end{variableblock}
\end{frame}

\begin{frame}{Introduction}
\begin{itemize}
\item Data availability in a Bayesian framework
\begin{itemize}
\item We could design an optimal classifier if we know
\begin{itemize}
\item $P(w_j)$ (priors)
\item $p({\rm x}|w_j)$ (class-conditional densities)
\end{itemize}
Unfortunately, we rarely have this complete information.
\end{itemize}
\item Design a classifier from training samples
\begin{itemize}
\item No problem with prior estimation
\item Samples are often too small for class-conditional estimation (large dimension of feature space)
\end{itemize}
\item Some priori information about the problem should be known.
\item Normality of $p({\rm x}|w_j)$
\begin{equation}
p({\rm x}|w_j)\sim N(\mu_j,\Sigma_j)\nonumber
\end{equation}
\end{itemize}
\end{frame}

\begin{frame}{Introduction}
\begin{itemize}
\item Parametric estimation techniques
\begin{itemize}
\item \textit{\color{mycolor1}Maximum-Likelihood Estimation (MLE)} and 
\item \textit{\color{mycolor2}Bayesian Estimations}
\end{itemize}
\item Some other approaches for parameter estimation
\begin{itemize}
\item Histogram based technique
\item Parzen-Rosenblatt window technique (Kernel/Window based technique)
\end{itemize}
\item Results are nearly identical, but approaches are different
\item Parameters in MLE are fixed but unknown.
\item Best parameters are obtained by maximizing the probability of obtaining the samples observed.
\item Bayesian methods view the parameters as random variables having some known distribution
\item In either approach, we use $P(w_i |{\rm x})$ for our classification rule.
\end{itemize}
\end{frame}

\begin{frame}{Difference between ML and Bayesian estimation}
\begin{itemize}
\item \textit{\color{mycolor1}Maximum-Likelihood Estimation (MLE)} 
\begin{itemize}
\item Views the parameters as quantities whose values are fixed but unknown.
\item We Estimate these values by maximizing the probability of obtaining the samples observed.
\end{itemize}
\item \textit{\color{mycolor2}Bayesian Estimations}
\begin{itemize}
\item Views the parameters as random variables having some known prior distribution.
\item We observe new samples and converts the prior to a posterior density.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{}
\begin{variableblock}{\centering \Large \textbf{\vspace{4pt}\newline Maximum Likelihood Estimation\vspace{4pt}}}{bg=slidecolor,fg=white}{bg=slidecolor,fg=white}
\end{variableblock}
\end{frame}

\begin{frame}{Maximum Likelihood Estimation}
\begin{itemize}
\item $C\rightarrow$ no. of classes
\item $\mathcal{D}_1,\mathcal{D}_2,\mathcal{D}_3,\ldots,\mathcal{D}_C$ (set of features for different classes)
\item $p({\rm x}|w_j)\rightarrow$ known parametric form
\begin{equation}
p({\rm x}|w_j)\sim N(\mu_j,\Sigma_j)\nonumber
\end{equation}
where $\mu_j$ is the mean vector, and $\Sigma_j$ is the co-variance matrix.
\item For parameter vector $\theta_j = \left[\mu_j,
\Sigma_j\right]^T$, the {\color{mycolor1}parametric probability distribution function} as
\begin{equation}
p({\rm x}|w_j)\equiv p({\rm x}|w_j,\theta_j)=p({\rm x}|\theta_j)\nonumber
\end{equation}
\item Here our objective is to use the information from the training samples in set $\mathcal{D}_j$ to obtain good estimates for the unknown parameter vector $\theta_j$.
\item We can apply MLE on individual set to estimate the parameters.
\end{itemize}
\end{frame}

\begin{frame}{Maximum Likelihood Estimation}
\begin{small}
\begin{itemize}
\item Let us assume the set $\mathcal{D}_j = \{{\rm x}_1,{\rm x}_2,\ldots,{\rm x}_n\}$ of independent
and identically distributed (i.i.d.) samples drawn from the
density $p({\rm x}|\theta_j)$ 
\item That means $\mathcal{D}_i$ does not provide any information about the parameter vector $\theta_j$ for $i\neq j$, \textit{i.e.}, samples from one class do not provide any information of the parameter vector of the \textit{\color{mycolor1}probability density function} of another class.
\item Thus, we can work with each class separately and omit the class labels ($j$), so that we write the probability density as  $p({\rm x}|\theta)$.
\item Thus, the probability of observing $\mathcal{D} =\{{\rm x}_1,{\rm x}_2,\ldots,{\rm x}_n\}$ is
\begin{equation}
p(\mathcal{D}| \theta ) =p({\rm x}_1|\theta)*p({\rm x}_2|\theta)*\cdots*p({\rm x}_n|\theta) = \prod\limits_{k = 1}^n {p({{\rm x}_k}|\theta )}\nonumber
\end{equation}
where $n$ is the number of data samples in set $\mathcal{D}$.
\item $p(\mathcal{D}|\theta)$ is also called the likelihood of $\theta$ with respect to the set of samples $\mathcal{D}$.
\end{itemize}
\end{small}
\end{frame}

\begin{frame}{{Maximum Likelihood Estimation}}
\begin{itemize}
\item The \textit{\color{mycolor1}maximum-likelihood estimation} of $\theta$ is, by definition, the value $\hat{\theta}$ that maximizes $p(\mathcal{D}|\theta)$.
\end{itemize}
\vspace{-12pt}
\begin{columns}
\begin{column}{6cm}
\vspace{-12pt}
\begin{figure}
\includegraphics[scale=0.58]{Ch0301}
\end{figure}
\end{column}
\begin{column}{5cm}
\begin{equation}
p(\mathcal{D}| \theta ) = \prod\limits_{k = 1}^n {p({{\rm x}_k}|\theta )}\nonumber
\end{equation}

\begin{align}
l(\theta)&=\ln p(\mathcal{D}|\theta)\nonumber\\
&=\sum\limits_{k = 1}^n {{\mathop{\rm \ln } \nolimits}~p({{\rm x}_k}|\theta )}\nonumber
\end{align}
{\color{mycolor2}Solution}\\
\begin{equation}
\hat \theta  = \arg \mathop {\max }\limits_\theta  l(\theta )\nonumber
\end{equation}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Maximum Likelihood Estimation-Optimal estimation}
\begin{itemize}
\item Let $\theta=(\theta_1,\theta_2,\ldots,\theta_p)^T$, and $\nabla_{\theta}$ be the gradient operator
\begin{equation}
{\nabla _\theta } = \left[ {\begin{array}{*{20}{c}}
{\frac{\partial }{{\partial {\theta _1}}}}\\
 \vdots \\
{\frac{\partial }{{\partial {\theta _p}}}}
\end{array}} \right]\nonumber
\end{equation}
\item $\Delta_{\theta}l(\theta)=0$
\item {\color{mycolor1}Example of a specific case}: Gaussian distribution
\item Multivariate normal population with $(\mu,\Sigma)$
\end{itemize}
\end{frame}

\begin{frame}{Gaussian case: Unknown $\mu$}
\begin{itemize}
\item $\sigma^2$ is known, only $\mu$ is unknown.
\begin{figure}
\includegraphics[scale=1]{Ch0302}\\
\includegraphics[scale=1]{Ch0303}~~~~~~~~~
\end{figure}
\item The maximum likelihood estimate for $\mu$ must satisfy
\begin{figure}
\includegraphics[scale=1]{Ch0304}
\end{figure}
\item Each of the $d$ component of $\hat{\mu}$ must vanish.
\begin{figure}
\begin{figure}
\includegraphics[scale=1]{Ch0305}
\end{figure}
\end{figure}
\end{itemize}
\end{frame}

\begin{frame}{Gaussian case: Unknown $\mu$ and $\Sigma$}
\begin{itemize}
\item $\theta_1 = \mu$ and $\theta_2=\sigma^2$ are unknown.
\item The log-likelihood of a single point is
\begin{figure}
\includegraphics[scale=1]{Ch0306}
\end{figure}
\item Derivative is
\begin{figure}
\includegraphics[scale=1]{Ch0307}
\end{figure}
\item After simplification
\begin{figure}
\includegraphics[scale=1]{Ch0308}~~~~~
\includegraphics[scale=1]{Ch0309}
\end{figure}
\end{itemize}
\end{frame}

\begin{frame}{Example}
\begin{itemize}
\item Estimate optimal parameter $\hat{\theta}$
\begin{equation}
p(x|\theta ) = \left\{ {\begin{array}{*{20}{c}}
{\theta {e^{ - \theta x}}}&{x \ge 0}\\
0&{otherwise}
\end{array}} \right.\nonumber
\end{equation}
using log-maximum likelihood estimation approach.
\end{itemize}
\end{frame}

\section{Non-parametric estimation}
\subsection{}

\begin{frame}{}
\begin{variableblock}{\centering \Large \textbf{\vspace{4pt}\newline Non-parametric parameter estimation\vspace{4pt}}}{bg=slidecolor,fg=white}{bg=slidecolor,fg=white}
\end{variableblock}
\end{frame}

\begin{frame}{Introduction}
\begin{itemize}
\setlength{\itemsep}{12pt}
\item We have already seen that for statistical pattern classification, density function are to be known for each class.
\item The type of density function, such as the normal or poison, are to be known to estimate the parameters of the densities called \textit{\color{mycolor1}parametric estimation}.
\item In most real problems, even the types of the density functions of interest are unknown.
\end{itemize}
\end{frame}

\begin{frame}{Introduction}
\begin{itemize}
\setlength{\itemsep}{8pt}
\item Looking at histograms, scatter plots or tables of the data may suggest that a particular type of class density may be used or some arbitrary density can be used.
\item Arbitrary density function can be estimated from the data samples using \textit{\color{mycolor1}nonparametric methods}.
\item  In addition, most of the classical parametric densities are
unimodal, whereas many practical problems involve multimodal densities.
\item Non-parametric methods can be used with arbitrary
distributions and without the assumption that the forms of
the underlying densities are known.
\end{itemize}
\end{frame}

\begin{frame}{Histogram Method}
\vspace{-0.5cm}
\begin{columns}
\begin{column}{5cm}
\begin{footnotesize}
\begin{itemize}
\item A very simple method is to
partition the space into a
number of equally-sized
cells (bins) and compute a
histogram.
\end{itemize}
\end{footnotesize}
\end{column}
\begin{column}{5cm}
\begin{figure}
\includegraphics[scale=0.7]{npe05}
\caption{Histogram in one dimension}
\end{figure}
\end{column}
\end{columns}
\begin{footnotesize}
\begin{itemize}
\item The estimate of the density at a point x becomes
\begin{equation}
p({\rm x})=\frac{k}{nV} \nonumber
\end{equation}
where $n$ is the total number of samples, $k$ is the number of samples in the bin that includes ${\rm x}$, and $V$ is the volume of that cell.
\item For 1-D feature, V is width of bin. Similarly for 2-D feature, V is the area of the bin.
\item Thumb rule to choose the number of intervals to be equal to the square root of the number of samples
\end{itemize}
\end{footnotesize}
\end{frame}

\begin{frame}{Histogram method}
\begin{figure}
\includegraphics[scale=0.1]{hist02.JPG}\\
\includegraphics[scale=0.1]{hist03.JPG}
\caption{{\scriptsize (a) The true normal density from which 50 random numbers were chosen. (b) A histogram of 50 normally distributed random numbers with six intervals. (c) A histogram of 50 normally distributed random numbers with three intervals. (d) A histogram of 50 normally distributed random numbers with 24 intervals.}}
\end{figure}
\end{frame}

\begin{frame}{Histogram method: Example}
\begin{footnotesize}
\textit{\color{mycolor1}Question:} Classification of samples using histograms and Bayes' Theorem\\
Use the following data to classify a sample with $x=7.5$, given that $P(A)=P(B)=0.5$. The following data are the values of feature $x$ for 60 randomly chosen samples from \\
Class A:
\begin{figure}
\includegraphics[scale=0.1]{hist04.JPG}
\end{figure}
Class B:
\begin{figure}
\includegraphics[scale=0.1]{hist05.JPG}
\end{figure}
\end{footnotesize}
\end{frame}

\begin{frame}{Histogram method: Solution}
\begin{figure}
\includegraphics[scale=0.15]{hist06.JPG}
\end{figure}
$P(A|7.5)=0.125$ and $P(B|7.5)=0.875$, so the sample should be classified into class $B$.
\end{frame}

\begin{frame}{2-D Histogram method}
\begin{itemize}
\item Histograms are not restricted to one-dimensional densities, but can be used in any number of dimensions.
\item $p(x,y)$ can be approximated by dividing both $x$ and $y$ into intervals, and determining the number of samples that fall within each rectangular histogram bin with dimensions $\Delta x$ and $\Delta y$.
\item The volume under the surface of this two-dimensional histogram is to be normalized to equal one, to yield an estimate of the density function $p(x,y)$
\item The histogram technique becomes impractical for spaces of high dimension.
\item The square root rule of thumb can be generalized to produce an \textit{\color{mycolor1}equal precision rule}. When there are $n$ features, the $(n+1)st$ root is used.
\end{itemize}
\end{frame}

\begin{frame}{Kernel and Window Estimators}
\begin{itemize}
\item The samples gives a very rough approximation to the true density function, namely a set of spikes or delta functions, one at each sample value, each with a very small width and a very large height.
\item The combined area of all the spikes is one.
\item Histogram based density approximation to  a continuous density function is not useful in decision making.
\item If the delta functions at each sample point are replaced by other function called \textit{\color{sc}Kernels} -- such as \textit{\color{sc}rectangles}, \textit{\color{sc}triangles}, or \textit{\color{sc}normal density functions}, which have been scaled so that their combined area equals one-their sum produces a smoother, more satisfactory estimate.
\end{itemize}
\end{frame}

\begin{frame}{Example}
\textit{\color{mycolor1}Question:} Using a triangle kernel.\\
Consider the data set with one feature $x$ and three samples at $x=1,2$, and $4$. We have decided to use a triangular kernel with a base of three units. Plot the estimated density function $p(x)$.\nocite{duda2012pattern}\nocite{gose1997pattern}
\end{frame}

\begin{frame}{Solution}
\begin{figure}
\includegraphics[scale=0.15]{hist09.JPG}
\end{figure}
\end{frame}

\begin{frame}{Non-parametric Density Estimation}
\begin{figure}
\includegraphics[scale=0.85]{npe02}
\end{figure}
\end{frame}

\begin{frame}{Non-parametric Density Estimation}
\begin{figure}
\includegraphics[scale=0.9]{npe03}
\end{figure}
\end{frame}

\begin{frame}{Non-parametric Density Estimation}
\begin{figure}
\includegraphics[scale=0.9]{npe04}
\end{figure}
\end{frame}

\begin{frame}{Non-parametric Methods}
\begin{figure}
\includegraphics[scale=0.8]{npe01}
\end{figure}
\end{frame}



\begin{frame}{Non-parametric Density Estimation}
\vspace{-0.5cm}
%\begin{itemize}
%\item Other methods for obtaining the regions for estimation:
%\begin{itemize}
%\item Shrink regions as some function of $n$, such as $V_n=1|\sqrt{n}$.
%This is the \textit{\color{mycolor1} Parzen window} estimation
%\item Specify $k_n$ as some function of $n$, such as $k_n=\sqrt{n}$. This is the \textit{\color{mycolor1}$k$-nearest neighbor} estimation
%\end{itemize}
%\begin{figure}
%\includegraphics[scale=1]{npe06}
%\end{figure}
%\end{itemize}
\begin{figure}
\includegraphics[scale=0.9]{npe06}
\end{figure}
\end{frame}

\begin{frame}{Parzen Window}
\begin{figure}
\includegraphics[scale=0.9]{npe07}
\end{figure}
\end{frame}

\begin{frame}{Parzen Window}
\vspace{-0.5cm}
\begin{figure}
\includegraphics[scale=0.9]{npe08}
\end{figure}
\end{frame}

\begin{frame}{Parzen Window}
\vspace{-0.5cm}
\begin{figure}
\includegraphics[scale=0.9]{npe09}
\end{figure}
\end{frame}

\begin{frame}{Parzen Window}
\begin{figure}
\includegraphics[scale=0.8]{npe10}
\end{figure}
\end{frame}
\begin{frame}{Parzen Window}
\begin{figure}
\includegraphics[scale=0.8]{npe11}
\end{figure}
\end{frame}

\begin{frame}{Parzen Window}
\begin{figure}
\includegraphics[scale=0.8]{npe12}
\end{figure}
\end{frame}

\begin{frame}{Parzen Window}
\vspace{-0.2cm}
\begin{figure}
\includegraphics[scale=0.9]{npe13}
\end{figure}
\end{frame}

\begin{frame}{$k-$Nearest Neighbors}
\begin{figure}
\includegraphics[scale=0.9]{npe14}
\end{figure}
\end{frame}

\begin{frame}{$k-$Nearest Neighbors}
\begin{figure}
\includegraphics[scale=0.85]{npe15}
\end{figure}
\end{frame}

\begin{frame}{$k-$Nearest Neighbors}
\begin{figure}
\includegraphics[scale=0.9]{npe16}
\end{figure}
\end{frame}

\begin{frame}{Non-parametric Methods}
\begin{figure}
\includegraphics[scale=0.9]{npe17}
\end{figure}
\end{frame}

\section{Nearest Neighbor Classification}
\subsection{}

\begin{frame}{}
\begin{variableblock}{\centering \Large \textbf{\vspace{4pt}\newline Nearest Neighbor Classification\vspace{4pt}}}{bg=slidecolor,fg=white}{bg=slidecolor,fg=white}
\end{variableblock}
\end{frame}

\begin{frame}{The Nearest Neighbor Classifier}
\begin{figure}
\includegraphics[scale=0.9]{knn02}
\end{figure}
\end{frame}

\begin{frame}{The Nearest Neighbor Classifier}
\begin{figure}
\includegraphics[scale=0.9]{knn03}
\end{figure}
\end{frame}

\begin{frame}{The Nearest Neighbor Classifier}
\vspace{-0.5cm}
\begin{figure}
\includegraphics[scale=0.9]{knn04}
\end{figure}
\end{frame}

\begin{frame}{The k-Nearest Neighbor Classifier}
\vspace{-0.5cm}
\begin{figure}
\includegraphics[scale=0.9]{knn05}
\end{figure}
\end{frame}

\begin{frame}{The k-Nearest Neighbor Classifier}
\begin{figure}
\includegraphics[scale=0.9]{knn06}
\end{figure}
\end{frame}

\begin{frame}{Nearest Neighbor Algorithm}
Learning Algorithm:
\begin{itemize}
\item Store training examples
\end{itemize}
Prediction Algorithm:
\begin{itemize}
\item To classify a new example $x$ by finding the training example $(x_i,y_i)$ that is nearest to $x$
\item Guess the class $y =y_i$
\end{itemize}
\end{frame}

\begin{frame}{Nearest Neighbor Algorithm}
\begin{itemize}
\item To classify a new input vector ${\rm x}$, examine the $k$-closest training data points to ${\rm x}$ and assign the object to the most frequently occurring class
\begin{figure}
\includegraphics[scale=0.5]{knn01}
\end{figure}
\item common values for $k$: 3, 5
\end{itemize}
\end{frame}

%\begin{frame}{The k-Nearest Neighbor Classifier}
%\begin{figure}
%\includegraphics[scale=0.9]{knn07}
%\end{figure}
%\end{frame}

\section{Distance Functions}
\subsection{}

\begin{frame}{}
\begin{variableblock}{\centering \Large \textbf{\vspace{4pt}\newline Distance Functions\vspace{4pt}}}{bg=slidecolor,fg=white}{bg=slidecolor,fg=white}
\end{variableblock}
\end{frame}

\begin{frame}{Distance Functions}
\begin{figure}
\includegraphics[scale=0.9]{knn08}
\end{figure}
\end{frame}

\begin{frame}{Distance Functions}
\begin{figure}
\includegraphics[scale=0.9]{knn09}
\end{figure}
\end{frame}

\begin{frame}{Distance Functions}
\begin{figure}
\includegraphics[scale=0.9]{knn10}
\end{figure}
\end{frame}

\begin{frame}{Feature Normalization}
\begin{figure}
\includegraphics[scale=0.9]{knn11}
\end{figure}
\end{frame}

\begin{frame}{Feature Normalization}
\begin{figure}
\includegraphics[scale=0.9]{knn12}
\end{figure}
\end{frame}

\begin{frame}{Problem on Probability Density Estimation}
\textit{Question:} Following sets of 2-D feature vectors from classes A and B are given
\begin{footnotesize}
\begin{equation}
\left\{ {\left( {\begin{array}{*{20}{c}}
1\\
1
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
1\\
3
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
2\\
1
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
2\\
1
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
{2.5}\\
2
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
3\\
3
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
4\\
1
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
4\\
2
\end{array}} \right)} \right\} \in A \nonumber
\end{equation}
\begin{equation}
\left\{ {\left( {\begin{array}{*{20}{c}}
3\\
2
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
4\\
3
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
4\\
3
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
4.5\\
3
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
{4}\\
4
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
6\\
3
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
4\\
6
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
7\\
3
\end{array}} \right)} \right\} \in B \nonumber
\end{equation}
\end{footnotesize}
Using rectangular window of size $3\times 3$, compute $p((3.5,3)^t|A)$ and $p((3.5,3)^t|B)$. Classify $(3.5,3)^t$ if $P(A)=1/3$ and $P(B)=2/3$.
\end{frame}

\begin{frame}{Problem on nearest neighbor rule}
Consider the following set of seven 2-dimensional feature vectors:
\begin{equation}
X_1=(1,0)^t,~X_2=(0,1)^t,~X_3=(0,-1)^t, \nonumber
\end{equation}
\begin{equation}
 ~X_4=(0,0)^t,~X_5=(0,2)^t,~X_6=(0,-2)^t, ~X_7=(-2,0)^t\nonumber
\end{equation}
If $X_1,X_2,X_3\in \omega_1$ and $X_4,X_5,X_6,X_7\in \omega_2$, sketch the decision boundary resulting from the nearest neighbor rule.
\end{frame}

%\begin{frame}{Feature Normalization}
%\begin{figure}
%\includegraphics[scale=0.9]{knn13}
%\end{figure}
%\end{frame}

%\begin{frame}{Introduction}
%\begin{itemize}
%\item 
%\end{itemize}
%\end{frame}
%


%\begin{frame}{Distance Metric}
%\begin{footnotesize}
%\begin{itemize}
%\item \textit{\color{mycolor1}Euclidean distance:} ${\rm a}=(a_1,a_2,\ldots,a_d)$ and ${\rm b}=(b_1,b_2,\ldots,b_d)$
%\begin{equation}
%{d_e}({\rm a},{\rm b}) = \sqrt {\sum\limits_{i = 1}^d {{{({b_i} - {a_i})}^2}} } \nonumber
%\end{equation} 
%where $d$ is the dimension of feature vector.
%\item Absolute differences:
%\begin{equation}
%{d_{cb}}(a,b) = \sum\limits_{i = 1}^d {\left| {{b_i} - {a_i}} \right|} \nonumber
%\end{equation}
%also called \textit{\color{mycolor1}city block distance}, the \textit{\color{mycolor1} Manhattan distance} or the \textit{\color{mycolor1} taxi-cab distance}
%\item \textit{\color{mycolor1} Maximum distance:}
%\[{d_m}(a,b) = \mathop {\max }\limits_{i = 1}^d \left| {{b_i} - {a_i}} \right|\]
%\end{itemize}
%\end{footnotesize}
%\end{frame}
%
%\begin{frame}{Distance Metric}
%\begin{footnotesize}
%\begin{itemize}
%\item \textit{\color{mycolor1} Minkowski distance:} ${\rm a}=(a_1,a_2,\ldots,a_d)$ and ${\rm b}=(b_1,b_2,\ldots,b_d)$
%\begin{equation}
%{d_r}(a,b) = {\left[ {\sum\limits_{i = 1}^d {{{\left| {{b_i} - {a_i}} \right|}^r}} } \right]^{{1 \mathord{\left/
% {\vphantom {1 r}} \right.
% \kern-\nulldelimiterspace} r}}} \nonumber
%\end{equation} 
%where $r$ is an adjustable parameter.
%\end{itemize}
%\end{footnotesize}
%\end{frame}

\section{References}
\subsection{}
\begin{frame}[allowframebreaks]{References}
\linespread{1}
\footnotesize
\printbibliography[heading=none]
\end{frame}
{
\nocite{Daugman1985}\nocite{Petkov1995}\nocite{Petkov1997}\nocite{Kruizinga1999}\nocite{Grigorescu2002}\nocite{Petkov2003}\nocite{Grigorescu2003}\nocite{Jain1991}
\setbeamertemplate{logo}{}
\makeatletter
\setbeamertemplate{footline}{
        \leavevmode%
  
  % First line.
  \hbox{%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=\beamer@decolines@lineup,dp=0pt]{}%
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.8\paperwidth,ht=\beamer@decolines@lineup,dp=0pt]{lineup}%
  \end{beamercolorbox}%
  } %
  % Second line.
  \hbox{%
  \begin{beamercolorbox}[wd=\paperwidth,ht=\beamer@decolines@linemid,dp=0pt]{linemid}%
  \end{beamercolorbox}%
  } %
  % Third line.
  \hbox{%
  \begin{beamercolorbox}[wd=.1\paperwidth,ht=\beamer@decolines@linebottom,dp=0pt]{}%
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.9\paperwidth,ht=\beamer@decolines@linebottom,dp=0pt]{linebottom}%
  \end{beamercolorbox}%
  }%
        }
\makeatother

\begin{frame}
\centering
\includegraphics[width=0.4\paperwidth]{queries.jpg}\\
\includegraphics[width=0.5\paperwidth]{thank_you.png}
\end{frame}
}